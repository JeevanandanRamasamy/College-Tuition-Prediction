# College Tuition Prediction

This project aims to predict college tuition fees based on various features related to colleges in the United States. The dataset contains information about multiple colleges, including academic factors, demographics, and financial attributes. The project includes several data preprocessing steps, exploratory data analysis, and a comparison of different machine learning models to identify the best approach for predicting tuition fees. After evaluating different models, **XGBoost** emerged as the best model, providing the most accurate predictions.

---

## Files Included

1. **colleges_train.csv**: This CSV file contains the training data for the project. It includes information about colleges, such as their admission rates, SAT scores, faculty composition, tuition fees, and more.
2. **colleges_test_features.csv**: This file contains the features of the colleges for which the tuition fees need to be predicted. The test data is preprocessed in a similar manner as the training data.
3. **final_project.html**: This file contains a detailed report of the entire project, including methodology, results, and visualizations. It presents a comprehensive summary of the analysis and model performance.
4. **final_project.ipynb**: The Jupyter notebook file used to run the code for this project. It includes all the steps from data loading, preprocessing, model training, and evaluation, to visualization. You can run this notebook to replicate the project results.
5. **predictions.csv**: This file contains the predicted tuition fees for the colleges in the test dataset, along with 90% confidence intervals for each prediction.

---

## Features

The dataset includes the following features (summarized for relevance to the project):
- **Demographics and Enrollment Data**: Information about the college’s student population, such as the total number of undergraduate students (ug), the gender distribution (ug_men, ug_women), racial and ethnic breakdowns (ug_white, ug_black, etc.), and the percentage of students who are 25 years or older (ug_25plus). It also includes data on the proportion of first-generation students (first_gen) and those receiving Pell Grants (pell_grant_rate) or federal loans (fed_loan_rate).
- **Academic and Faculty Data**: Several features related to the college’s academic rigor, including SAT scores (satv25, satv50, satv75, satm25, satm50, satm75), degree offerings in various fields (e.g., math_deg, faculty_salary), and specific disciplines (e.g., engi_deg, bio_deg, compsci_deg). The dataset also contains information on faculty composition (ft_faculty_rate) and faculty salaries (faculty_salary).
- **Financial Data**: Includes financial attributes such as endowment (endowment), room and board costs (roomboard), and book supply costs (booksupply).
- **Other Factors**: Additional factors that could affect tuition include the college’s admission rate (adm_rate), the number of degrees offered (e.g., law_deg, health_deg, business_deg), and the proportion of students pursuing specific degrees (e.g., bio_deg, sci_deg, law_deg).

---

## Model Performance

Several machine learning models were tested throughout the project, including:
- **Linear Regression**
- **Random Forest Regressor**
- **XGBoost** (the final model used)

XGBoost was found to be the most accurate model, with the best performance in terms of Mean Squared Error (MSE) and predictive accuracy on the test dataset. The model was trained using bootstrapping and evaluated using confidence intervals for predictions. The predictions generated by the XGBoost model are stored in the **predictions.csv** file, which contains the predicted tuition values along with their 90% confidence intervals.

---

## Bootstrapping and Model Accuracy

To further enhance the accuracy of our predictions, we utilized a technique known as **bootstrapping** during the training of our model. **Bootstrapping** is a statistical method that involves generating multiple datasets through resampling with replacement from the original training data. This process allows the model to be trained on different subsets of data, helping it to generalize better and reduce the likelihood of overfitting.

Specifically, bootstrapping allowed us to:
- **Increase Variability in Training**: By resampling the data with replacement, each bootstrapped dataset slightly varied, giving the model exposure to a broader representation of the data.
- **Reduce Overfitting**: The resampling ensured that the model wasn’t memorizing the data (overfitting), but rather learning generalized patterns that applied to unseen data.
- **Improve Confidence in Predictions**: After training the model on the multiple bootstrapped datasets, we could aggregate the results to generate a more reliable prediction with lower variance. This also allowed us to calculate confidence intervals for our predictions, giving us a measure of certainty about the model’s outputs.

### Impact on Performance

The bootstrapping method significantly contributed to the accuracy and reliability of the model, making it the best-performing model in a class of 80+ students. The model’s ability to produce stable and consistent predictions, validated by confidence intervals, gave us a competitive edge and demonstrated the power of ensemble learning and statistical resampling methods in improving model performance.

---

## How to Use

To use this project, you can either explore the **final_project.ipynb** Jupyter notebook to replicate the entire analysis or view the **final_project.html** report for a summary of the results. The **predictions.csv** file contains the predicted tuition values, which can be used for further analysis or insights.

To generate predictions for your own dataset or perform further model evaluation, follow these steps:
1. Load the necessary data files (`colleges_train.csv`, `colleges_test_features.csv`).
2. Preprocess the data (scaling, feature engineering, etc.).
3. Train the model using XGBoost or other models if desired.
4. Evaluate the model and visualize the results.
5. Generate predictions for the test data and save them in a CSV file.


